---
title: "Spectral Bridges"
subtitle: "Scalable Spectral Clustering free from hyperparameters"
date: 19/06/2024
date-modified: last-modified
author:
  - name: Félix Laplante
    email: flheight0@gmail.com
    affiliations:
      - Université de Paris Saclay
  - name: Christophe Ambroise
    corresponding: true
    email: christophe.ambroise@univ-evry.fr
    url: https://computo.sfds.asso.fr
    orcid: 0000-0002-8148-0346
    affiliations:
      - name: Université Paris-Saclay, CNRS, Univ Evry, 
        department: Laboratoire de Mathématiques et Modélisation d'Evry
        address: 23 boulevard de France
        city: Evry-Courcouronnes
        country: France
description: |
  This document provides a template based on the quarto system for contributions to Computo. The github repository in itself provides a specific quarto extension useful for authors (and editors!).
keywords: [spectral clustering, vector quantization, scalable, non-parametric]
doi: 10.xxxx/xxx-xxx
citation:
  type: article-journal
  container-title: "Computo"
  doi: "10.xxxx/xxx-xxx"
  url: "https://github.com/computorg/computo-quarto-extension"
  issn: "2824-7795"
bibliography: references.bib
google-scholar: true
github-user: cambroise
repo: "spectral-bridges"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default 
  computo-pdf:  default
editor: 
  markdown: 
    wrap: 72
---

# Abstract

In this paper, Spectral-Bridges, a novel clustering algorithm, is introduced. This algorithm builds upon the traditional k-means and spectral clustering frameworks by subdividing data into small Voronoï regions, which are subsequently assessed for their connectivity. Drawing inspiration from Support vector machine margin concept, a non-parametric clustering approach is proposed, building  an affinity margin between each pair of Voronoï regions. This approach is characterized by minimal hyperparameters and delineation of intricate, non-convex cluster structures.

The numerical experiments underscore Spectral-Bridges as a fast, robust, and versatile tool for sophisticated clustering tasks spanning diverse domains. Its efficacy is observed to extend to large-scale scenarios encompassing both real-world and synthetic datasets.


# Introduction

Clustering is a fundamental technique for exploratory data analysis, organizing a set of objects into distinct homogeneous groups known as clusters. It is extensively utilized across various fields, such as biology for gene expression analysis [@Eisen1998], social sciences for community detection in social networks [@latouche2011], and psychology for identifying behavioral patterns. Clustering is often employed alongside supervised learning as a pre-processing step, helping to structure and simplify data, thus enhancing the performance and interpretability of subsequent predictive models [@Verhaak2010]. Additionally, clustering can be integrated into supervised learning algorithms, such as mixture of experts [@jacobs1991adaptive], as part of a multi-objective strategy.

There are numerous approaches to clustering, each defined by how similarity between objects is measured, either through a similarity measure, a distance metric, or a statistical model.

Density-based methods identify regions within the data with a high concentration of points, corresponding to the modes of the joint density. A notable non-parametric example of this approach is DBSCAN [@ester1996density]. In contrast, model-based clustering, such as Gaussian mixture models, represents a parametric approach to density-based methods. Model-based clustering assumes that the data is generated from a mixture of underlying probability distributions, typically Gaussian distributions. Each cluster is viewed as a component of this mixture model, and the Expectation-Maximization (EM) algorithm is often used to estimate the parameters. This approach provides a probabilistic framework for clustering, allowing for the incorporation of prior knowledge and the ability to handle more complex cluster shapes and distributions [@mclachlan2000finite].

Geometric approaches, such as k-means [@macqueen1967some], are distance-based methods that aim to partition data by optimizing a criterion reflecting group homogeneity. The k-means++ algorithm [@arthur2007kmeanspp] enhances this approach by providing fast, convenient, and interpretable results. However, a key limitation of these methods is the assumption of linear boundaries between clusters, implying that clusters are convex. To address non-convex clusters, the kernel trick can be applied, allowing for a more flexible k-means algorithm. This approach is comparable to spectral clustering in handling complex cluster boundaries [@dhillon2004kernel]. The k-means algorithm can also be interpreted within the framework of model-based clustering under specific assumptions [@govaert2003clustering], revealing that it is essentially a special case of the more general Gaussian mixture models, where clusters are assumed to be spherical Gaussian distributions with equal variance.

Graph-based methods represent data as a graph, with vertices symbolizing data points and edges weighted to indicate the affinity between these points. Spectral clustering can be seen as a relaxed version of the graph cut algorithm [@shi2000normalized]. However, traditional spectral clustering faces significant limitations due to its high time and space complexity, greatly hindering its applicability to large-scale problems [@von2007tutorial].


The method we propose aims to find non-convex clusters in large datasets, without relying on parametric model,  by using spectral clustering based on an affinity that characterizes the local density of the data. The algorithm described in this paper draws from numerous clustering approaches. The initial intuition is to detect high-density areas. To this end, vector quantization is used to divide the space into a Voronoi tessellation. An original geometric criterion is then employed to detect pairs of Voronoi regions that are either distant from each other or separated by a low-density boundary. Finally, this affinity measure is considered as the weight of an edge in a complete graph connecting the centroids of the tessellation, and a spectral clustering algorithm is used to find a partition of this graph. The only parameters of the algorithm are the number of Voronoï Cells and the number of clusters.   

The paper begins with a section dedicated to presenting the context and related algorithms, followed by a detailed description of the proposed algorithm. Experiments and comparisons with reference algorithms are then conducted on both real and synthetic data.

# Related Work


Spectral clustering is a graph-based approach that computes the eigen-vectors of the graph's Laplacian matrix. This technique transforms the data into a lower-dimensional space, making the clusters more discernible. A standard algorithm like k-means is then applied to these transformed features to identify the clusters [@von2007tutorial]. Spectral clustering enables capturing complex data structures and discerning clusters based on the connectivity of data points in a transformed space, effectively treating it as a relaxed graph cut problem.

Classical spectral clustering involves two phases: construction of the affinity matrix and  eigen-decomposition. Constructing the affinity matrix requires $O(n^2d)$ time  and $O(n)$ memory, while eigen-decomposition demands $O(n^3)$ time and $O(n^2)$ memory, where $n$ is the data size and $d$ is the dimension. As $n$ increases, the computational load escalates significantly [@von2007tutorial].

To mitigate this computational burden, one common approach is to sparsify the affinity matrix and use sparse eigen-solvers, reducing memory costs but still requiring computation of all original matrix entries [@von2007tutorial]. Another strategy is sub-matrix construction. The Nyström method randomly selects $m$ representatives from the dataset to form an $n\times m$ affinity sub-matrix [@chen2010parallel]. Cai et al. extended this with the landmark-based spectral clustering method, which uses k-means to determine $m$ cluster centers as representatives [@cai2014large]. Ultra-scalable spectral clustering (U-SPEC) employs a hybrid representative selection strategy and a fast approximation method for constructing a sparse affinity sub-matrix [@huang2019ultra]. 

Other approaches uses the properties of the small initial cluster for the affinity computation.   Clustering Based on Graph of Intensity Topology (GIT)  estimates a global topological graph (topo-graph) between local clusters [@gao2021git]. I then uses the Wasserstein Distance between predicted and prior class proportions to automatically cut noisy edges in the topo-graph and merge connected local clusters into final clusters. 

The issue of characterizing the affinity between two clusters to create an edge weight is central to the efficiency of a spectral clustering algorithm operating from a submatrix.

Notice that the  clustering robustness of many Spectral clustering algorithm heavily relies on the proper selection of kernel parameter, which is difficult to find without prior knowledge [@ng2001spectral].



<!--
The approach using k-means to determine $m$ clusters and then creating a graph from these clusters is similar to certain penalized versions of Kohonen self-organizing maps, where the graph nodes are the centers of the $m$ clusters and the edge weights are related to the distance between the centroids.


$$
\text{Objective Function} = \sum_{\text{data points}} \left( \text{reconstruction error} \right) + \lambda \sum_{\text{neighbors}} (\text{weight}_{i} - \text{weight}_{j})^2
$$


Ainsi le terme de pénalité utilisé pour les carte de Kohonen pourrais être utilisé par caractériser la similiarté entre deux
-->


# Spectral bridges

The proposed algorithm uses K-means centroids for vector quantization defining Voronoi region, and a strategy is proposed to link these regions, with an "affinity" gauged in terms of minimal margin between pairs of classes. These affinities are considered as weight of edges defining a completely connected graph whose vertices are the regions. Spectral clustering on the region provide a partition of the input space. The sole parameters of the algorithm are the number of Voronoi region and the number of final cluster. 


## Bridge gain affinity

The basic idea involves calculating the difference in inertia achieved
by projecting onto a segment connecting two centroids, rather than using
the two centroids separately (See Figure @fig-balls-vs-bridge). If the difference is small, it suggests a low density between the classes. Conversely, if this diffrence is large,
it indicates that the two classes may reside within the same densely
populated region.

<!-- ::: {#fig-balls-vs-bridge} -->
<!-- ::::{.columns} -->
<!-- ::: {.column with="50%"} -->
<!-- ![](figures/balls.pdf){width=150,height=150} -->
<!-- ::: -->
<!-- ::: {.column with="50%"} -->
<!-- ![](figures/bridge.pdf){width=150,height=150} -->
<!-- ::: -->
<!-- :::: -->
<!-- Balls (left) versus Bridge (right). The inertia of each structure is the sum of the squared distances represented by grey lines.   -->
<!-- ::: -->


::: {#fig-balls-bridge layout-ncol=2}
![](figures/balls.pdf){width=50%}

![](figures/bridge.pdf){width=50%}

Balls (left) versus Bridge (right). The inertia of each structure is the sum of the squared distances represented by grey lines.
:::


The inertia of two balls $k$ and $l$ is 
$$
I_{kl} = \sum_{i\in k} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2  + \sum_{i\in l} \|\boldsymbol x_i - \boldsymbol \mu_l\|^2.
$$ 
The inertia of a bridge between $k$ and $l$ is defined as $$
B_{kl} = \sum_{i\in kl} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2,
$$ where $$
\boldsymbol p_{kl}(\boldsymbol x_i) = \boldsymbol \mu_{k} + t_i(\boldsymbol \mu_{l} - \boldsymbol \mu_{k}),
$$ with $$
t_i  = \min\left(1, \max\left(0, \frac{\langle \boldsymbol x_i - \boldsymbol \mu_k | \boldsymbol \mu_l - \boldsymbol \mu_k\rangle}{\|  \boldsymbol \mu_l - \boldsymbol \mu_k \|^2}\right)\right). 
$$ 

The normalized average of the difference betwenn Bridge and balls inertia is (See [Appendix](#gain)) 
$$
\frac{B_{kl}- I_{kl}}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2} = \frac{\sum_{\boldsymbol{i} \in k} \langle \boldsymbol{x_i} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k \rangle_+^2  \sum_{\boldsymbol{i} \in l} \langle \boldsymbol{x_i} - \boldsymbol{\mu}_l \vert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l\rangle_+^2}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^4}.
$$


From this reduction, we define the bridge affinity between centroids $k$
and $l$ as: 
$$
a_{kl}=
\begin{cases}
0, & \text{ if } k=l,\\
\sqrt{\frac{B_{kl}- I_{kl}}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2}}, & \text{otherwise}.
\end{cases}
$$ 


The basic intuition behind this affinity is that $t_i$ represents the
relative position of the projection of $\boldsymbol x_i$ on the segment
$[\boldsymbol \mu_k,\boldsymbol \mu_l]$. For each $\boldsymbol x_i$ an
affinity value $\alpha_i$ is defined as $$
\alpha_i =
\begin{cases}
t_i, &  \text{ if } t_i\in[0,1/2]\\
1-t_i, & \text{ if } t_i\in]1/2,1],
\end{cases}
$$ This value represents the relative position on the segment, with the
centroid of the class to which $\boldsymbol x_i$ belongs as the starting
point.

The boundary that separates the two clusters defined by centroids
$\boldsymbol \mu_k$ and $\boldsymbol \mu_l$ is a hyperplane. This
hyperplane is orthogonal to the line segment connecting the centroids
and intersects this segment at its midpoint.

If we consider all points $\boldsymbol x_i \in kl$ which are not
projected on centroids but somewhere on the segments, The distance from
a point to the hyperplane is large, 
$$
\|\boldsymbol p_{kl}(\boldsymbol x_i) - \boldsymbol \mu_{kl}\| = (1/2-\alpha_i) \| \boldsymbol \mu_k-\boldsymbol \mu_l \|.
$$
This distance is similar to the concept of margin in Support Vector Machine [@Cortes1995].


When the $\alpha_i$ values are small (close to zero since $\alpha_i\in [0,1/2]$), the margins to the hyperplane
are large, indicating a low density between the classes. Conversely, if
the margins are small, it suggests that the two classes may reside
within the same densely populated region. Consequently, the sum of the
$\alpha_i$ or $\alpha_i^2$ increases with the density of the region
between the classes. 

Note that the criterion is local and indicates the relative difference in densities between the balls and the bridge, rather than evaluating a global score for the densities of the structures.

## Algorithm

The Spectral Bridges algorithm first identifies local clusters to define Voronoi regions, computes edges with affinity weights between these regions, and ultimately cuts edges between regions with low inter-region density to determine the final clusters (See Algorithm @alg-spectral-bridges and Figure @fig-steps).

```pseudocode
#| label: alg-spectral-bridges
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Spectral Bridges}
\begin{algorithmic}
\Procedure{SpectralBridges}{$X, k, m$} \Comment{$X$: input dataset, $k$: number of clusters, $m$: number of Voronoi regions}
    \State \textbf{Step 1: Vector Quantization}
    \State $centroids \gets$ \Call{KMeans}{$X, m$} \Comment{Initial centroids using k-means++}
    \State $voronoiRegions \gets$ \Call{Subdivide}{$X, centroids$} \Comment{Subdivide data into Voronoi regions}
       \State \textbf{Step 2: Affinity Matrix Computation} $A = \{a_{kl}\}$
        \State \textbf{Step 3: Spectral Clustering} \Comment{Affect each region to a cluster}
        \State $labels \gets$ \Call{SpectralClustering}{$A, k$}
      \State \textbf{Step 4: Propagate} \Comment{Each data point is affected to the cluster of its region}
      \State $clusters \gets$ \Call{Propagate}{$X, labels, centroids$}
     \State \Return $clusters$ \Comment{Cluster labels for data points in $X$}
\EndProcedure
\end{algorithmic}
\end{algorithm}
```




::: {#fig-steps layout-ncol=3}
![Vector quantization](figures/spectral-briges-1.pdf)

![Affinity computation](figures/spectral-briges-2.pdf)

![Spectral clustering](figures/spectral-briges-3-4.pdf)

Illustration of the Spectral bridges algorithm with the Iris dataset (first principal plane). Vector quantization (Step 1 of Algorithm @alg-spectral-bridges), Affinity computation (Step 2 of Algorithm @alg-spectral-bridges), Spectral clustering and spreading (Step 3-4 of Algorithm @alg-spectral-bridges).
:::


# Numerical experiments


In this section, we present the results obtained from testing our algorithm on various datasets, both small and large scale, including real-world and well-known synthetic datasets. These experiments assess the accuracy, time and space complexity, ease of use, robustness, and adaptability of our algorithm. We compare **Spectral-Bridges** (**SB**) against several state-of-the-art methods, including **k-means++** (**KM**) [@macqueen1967some; @arthur2007kmeanspp], **Expectation-Maximization** (**EM**) [@dempster1977maximum], **Ward Clustering** (**WC**) [@ward1963hierarchical], and **DBSCAN** (**DB**) [@ester1996density]. This comparison establishes baselines across centroid-based clustering algorithms, hierarchical methods, and density-based methods. We evaluate the algorithms on both raw and PCA-processed data with varying dimensionality. For synthetic datasets, we introduce Gaussian and/or uniform noise to evaluate the robustness of our algorithm.

## Real-world Data

- **MNIST**: A large dataset containing 60,000 handwritten digit images in ten balanced classes, commonly used for image processing benchmarks. Each image consists of $28 \times 28 = 784$ pixels.
- **UCI ML Breast Cancer Wisconsin**: A dataset featuring computed attributes from digitized images of fine needle aspirates (FNA) of breast masses, used to predict whether a tumor is malignant or benign.

## Synthetic Data

- **Impossible**: A synthetic dataset designed to challenge clustering algorithms with complex patterns.
- **Moons**: A two-dimensional dataset with two interleaving half circles.
- **Circles**: A synthetic dataset of points arranged in two non-linearly separable circles.
- **Smile**: A synthetic dataset with points arranged in the shape of a smiling face, used to test the separation of non-linearly separable data.

### Datasets Summary & Class Balance

| **Dataset**    | **#Dims** | **#Samples** | **#Classes** | **Class Proportions**                     |
| -------------- | --------- | ------------ | ------------ | ----------------------------------------- |
| MNIST          | 784       | 60000        | 10           | 9.9%, 11.2%, 9.9%, 10.3%, 9.7%, 9%, 9.9%, 10.4%, 9.7%, 9.9% |
| Breast Cancer  | 30        | 569          | 2            | 37.3%, 62.7%                              |
| Impossible     | 2         | 3594         | 7            | 24.8%, 18.8%, 11.3%, 7.5%, 12.5%, 12.5%, 12.5% |
| Moons          | 2         | 1000         | 2            | 50%, 50%                                  |
| Circles        | 2         | 1000         | 2            | 50%, 50%                                  |
| Smile          | 2         | 1000         | 4            | 25%, 25%, 25%, 25%                        |

Table: Datasets Summary & Class Balance


Class proportions are presented in ascending order starting from label $0$.

## Metrics

To evaluate the performance of our clustering algorithm, we use the Adjusted Rand Index (**ARI**) [@halkidi2002cluster] and Normalized Mutual Information (**NMI**) [@cover1991information]. ARI measures the similarity between two clustering results, ranging from $-0.5$ to $1$, with $1$ indicating perfect agreement. NMI ranges from $0$ to $1$, with higher values indicating better clustering quality. In some tests, we also report the variability of scores across multiple runs due to the random initialization in k-means, though k-means++ generally provides stable and reproducible results.

## Platform

All experiments were conducted on an Archlinux machine with Linux 6.9.3 Kernel, 8GB of RAM, and an AMD Ryzen 3 7320U processor.

## Hyperparameter settings

We set the hyperparameters of our algorithm to "reasonable" values based on the size of each dataset and the number of clusters. A larger number of clusters typically suggests that a higher value for the Voronoï regions is optimal. Conversely, using a high number of Voronoï regions for a small dataset might result in nearly empty regions that do not adequately represent any local structure. We will discuss our method's sensitivity to this parameter, but at no point did we use labels to determine its empirically optimal value.

For other algorithms, such as DBSCAN, we used labels to determine the best hyperparameter values to compare our method against the "best case scenario", thus putting our algorithm at a voluntary disadvantage.

## Accuracy

We first evaluated our algorithm's accuracy on the MNIST dataset. Metrics were collected to compare our method with k-means++, EM, and Ward clustering. Metric were estimated by taking the empirical average over $10$ consecutive runs with the same random seed for each method. Since our computational capabilites were too limited, we sampled $20000$ (one third) data points at random. This sample is not fixed and changes for each iteration.

Let $h$ denote the embedding dimension of the dataset. We tested our method on the raw MNIST dataset without preprocessing ($h = 784$) and after reducing its dimension using PCA to $h \in \{8, 16, 32, 64\}$ (see fig.1).

![ARI and NMI scores of **k-means++** (blue), **EM** (green), **Ward Clustering** (red), and **Spectral-Bridges** (purple) on PCA embedding and full MNIST](figures/mnist_summary.pdf)

For visualization purposes, we projected with UMAP the predicted clusters from our algorithm and other methods to compare them against the ground truth labels to better understand the cluster shapes (see table 2). Note that the projection was not used in our experiments as an embedding, and thus does not play any role in the clustering process itself. As a matter of fact, the embedding used was obtained with PCA, $h = 64$. Note that the label colors match the legend only in the case of the ground truth data. Indeed, the ordering of the labels have no impact on clustering quality.

::: {#fig-MNIST layout-ncol=3}

![k-means++](figures/KMumap.pdf)

![Spectral-Bridges](figures/SBumap.pdf)

![Ground Truth](figures/GTumap.pdf)


UMAP projection of predicted clusters against the ground truth labels.
:::

We also put our algorithm to the test against the same competitors using scikit-learn's UCI Breast Cancer data. Once again, our method performs well although the advantage is not as obvious in this case. However, in none of our tests has it ranked worse than k-means++. The results are displayed as a boxplot generated from $200$ iterations of each algorithm using a different seed, in order to better grasp the variability lying in the seed dependent nature of the k-means++, Expectation Maximization and Spectral-Bridges algorithms.
![ARI and NMI scores of **k-means++** (blue), **EM** (green), **Ward Clustering** (red), and **Spectral-Bridges** (purple) on the UCI Breast Cancer dataset](figures/cancer_summary.pdf)

Since we expect our algorithm to shine at discerning complex and intricate cluster structures, we collected an array four toy datasets illustrated below.

::: {#fig-ToyDatasets layout-ncol=4}

![Impossible](figures/impossible.pdf)

![Moons](figures/moons.pdf)

![Circles](figures/circles.pdf)

![Smile](figures/smile.pdf)

Visualization of the four toy datasets.
:::

We once again benchmark multiple algorithms including ours in the exact same manner as for the UCI Breast Cancer data. Results show our method outperforms all tested algorithms (DBSCAN, k-means++, Expectation Maximization and Ward Clustering) while needing few hyperparameters. In this regard, as discussed previously, DBSCAN's parameters have been set using the labels to illustrate a best case scenario, although, in practice, one can expect worse results. Despite this, Spectral-Bridge still manages to better encompass the clusters' structure.

![ARI and NMI scores of Spectral-Bridges and competitors on standard synthetic toy datasets](figures/synthetic_summary.pdf)

## Noise robustness

To evaluate the noise robustness of our algorithm, we imagined two setups : introducing Gaussian distributed perturbations and concatenating uniformly distributed points on a predefined rectangular region (whose size is determined by the span of the dataset) to a given dataset. As illustrated belew, in both of theses cases, our tests show our method is quite insensitive to noise in those scenarios.

Here are three representations of our algorithm's predicted cluster centers displayed as colored dots, while the each point of the Impossible dataset is shown as a small black dot. In the first graph, the dataset is not modified. In the second, we added $n=250$ uniformly distributed samples and in the last we used Gaussian noise perturbations with $\sigma = 0.1$.

::: {#fig-NoiseRobustness layout-ncol=3}

![Clean](figures/clean_impossible.pdf)

![Uniform noise](figures/uniform_noise_impossible.pdf)

![Gaussian noise](figures/gaussian_noise_impossible.pdf)

Effects of noise on Spectral-Bridges's output.
:::

# Conclusive remarks

Possibility to kernelize


# Appendix  {.appendix}

## Derivation of the bridge affinity {#gain}


Notice that $B_{kl}$, the bridge inertia between centroids $k$ and $l$,  can be expressed as the sum of three terms:

$$
\begin{aligned}
B_{kl} &=& \sum_{i \mid t_i=0} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2  + \sum_{i \mid t_i=1} \|\boldsymbol x_i - \boldsymbol \mu_l\|^2 + \sum_{i \mid t_i\in ]0,1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2.
\end{aligned}
$$

The last term may be decomposed in two parts

$$
\begin{aligned}
\sum_{i \mid t_i\in ]0,1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 &= &\sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 + \sum_{i \mid t_i\in [\frac{1}{2},1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2\\
\end{aligned}
$$

and each part further decomposed using Pythagore 
$$
\begin{aligned}
\sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 &=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol \mu_k - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2\\
&=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|t_i (\boldsymbol \mu_k - \boldsymbol \mu_{l})\|^2,
\end{aligned}
$$

$$
\begin{aligned}
\sum_{i \mid t_i\in ]\frac{1}{2},1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 &=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol \mu_l\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol \mu_l - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2\\
&=& \sum_{i \mid t_i\in ]\frac{1}{2},1[} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|(1-t_i) (\boldsymbol \mu_k - \boldsymbol \mu_{l})\|^2
\end{aligned}
$$

Thus $$
\begin{aligned}
B_{kl}- I_{kl} &=&  \sum_{i \mid t_i\in ]0,\frac{1}{2}[} t_i^2 \|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2 + \sum_{i \mid t_i\in ]\frac{1}{2},1[} (1-t_i)^2 \|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2,\\
\frac{B_{kl}- I_{kl}}{\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2} &=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} t_i^2  + \sum_{i \mid t_i\in ]\frac{1}{2},1[} (1-t_i)^2, \\
\frac{B_{kl}- I_{kl}}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2} &=& \frac{\sum_{\boldsymbol{i} \in k} \langle \boldsymbol{x_i} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k \rangle_+^2  \sum_{\boldsymbol{i} \in l} \langle \boldsymbol{x_i} - \boldsymbol{\mu}_l \vert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l\rangle_+^2}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^4}.
\end{aligned}
$$


# References {.unnumbered}

::: {#refs}
:::

# Session information {.appendix .unnumbered}

```{r session-info}
sessionInfo()
```
