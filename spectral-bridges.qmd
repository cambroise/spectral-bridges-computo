---
title: "Spectral Bridges"
subtitle: "Scalable Spectral Clustering free from hyperparameters"
date: 19/06/2024
date-modified: last-modified
author:
  - name: Félix Laplante
    email: flheight0@gmail.com
    affiliations:
      - Université de Paris Saclay
  - name: Christophe Ambroise
    corresponding: true
    email: christophe.ambroise@univ-evry.fr
    url: https://computo.sfds.asso.fr
    orcid: 0000-0002-8148-0346
    affiliations:
      - name: Université Paris-Saclay, CNRS, Univ Evry, 
        department: Laboratoire de Mathématiques et Modélisation d'Evry
        address: 23 boulevard de France
        city: Evry-Courcouronnes
        country: France
description: |
  This document provides a template based on the quarto system for contributions to Computo. The github repository in itself provides a specific quarto extension useful for authors (and editors!).
keywords: [spectral clustering, vector quantization, scalable, non-parametric]
doi: 10.xxxx/xxx-xxx
citation:
  type: article-journal
  container-title: "Computo"
  doi: "10.xxxx/xxx-xxx"
  url: "https://github.com/computorg/computo-quarto-extension"
  issn: "2824-7795"
bibliography: references.bib
google-scholar: true
github-user: cambroise
repo: "spectral-bridges"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default 
  computo-pdf:  default
---

# Abstract

In this paper, Spectral-Bridges, a novel clustering algorithm, is presented. This algorithm builds upon the traditional k-means and spectral clustering frameworks by subdividing data into small Voronoi regions, which are subsequently assessed for their connectivity. Drawing inspiration from Ward linkage, a non-parametric clustering approach is embraced by the Spectral-Bridges algorithm. This approach is characterized by minimal hyperparameters and intuitive usability, thereby augmenting adaptability and enabling the delineation of intricate, non-convex cluster structures.

Both global and local data arrangements are aimed to be discerned by Spectral-Bridges in a scale-invariant manner. K-means centroids are leveraged for subgroup initialization, and a strategy is proposed to link these regions, with the "reward" gauged in terms of newly captured variance achievable by connecting them through a projected data segment, referred to as a bridge.

The empirical results underscore Spectral-Bridges as a fast, robust, and versatile tool for sophisticated clustering tasks spanning diverse domains. Its efficacy is observed to extend seamlessly to large-scale scenarios encompassing both real-world and synthetic datasets.


# Introduction

<!--Interest of clustering-->
Clustering is a fundamental technique for exploratory data analysis. It partition a set of objects into a certain number of homogeneous groups, each referred to as a cluster.  It is extensively utilized across diverse fields such  biology, social sciences, and psychology. Clustering is frequently employed in conjunction with supervised learning as a pre-processing step, where it helps to structure and simplify data, thereby enhancing the performance and interpretability of subsequent predictive models.



<!-- Importance of defining "similar" -->


<!-- Clustering approaches -->

There are numerous approaches to clustering, each defined by how similarity between objects is measured, either through a similarity measure or more strictly through a distance metric.

Density-based methods identify regions within the data that have a high concentration of points, corresponding to the modes of the joint density. A notable non-parametric example of this approach is DBSCAN [@ester1996density]. In contrast, model-based clustering, such as Gaussian mixture models, represents a parametric approach to density-based methods.

Geometric approaches, such as kmeans [@macqueen1967some]  are distance-based and aim to partition the data in a way that optimizes a criterion reflecting group homogeneity.

Graph-based methods treat data as a graph, with vertices representing data points and edges weighted to reflect the affinity between these points.


<!--

Clustering is a fundamental problem in the field of data mining and machine learning [1], whose purpose is to partition a set of objects into a certain number of homogeneous groups, each referred to as a cluster. 
Out of the large number of clustering algorithms that have been developed, spectral clustering in recent years has been gaining increasing attention due to its promising ability in dealing with non- linearly separable datasets [2], [3], [4], [5]. However, a critical limitation to conventional spectral clustering lies in its huge time and space complexity, which significantly restricts its application to large-scale problems.
Conventional spectral clustering typically consists of two time- and memory-consuming phases, namely,
-->



# Background


Spectral clustering is a graph-based approach that computes the eigenvectors of the graph's Laplacian matrix. This technique transforms the data into a lower-dimensional space, making the clusters more discernible. A standard algorithm like k-means is then applied to these transformed features to identify the clusters[@von2007tutorial].

It enables to capture complex data structures and discern clusters based on the connectivity of data points in a transformed space.
Notice that spectral clustering can be seen as a relaxed graph cut problem. 






# Spectral bridges

## Bridge gain


The affinity matrix
Let $A = (a_{kl})_{1 \leq k,l \leq n}$ be the  affinity matrix:

$$
a_{kl} = \Big[ \frac{\sum_{\boldsymbol{x} \in P_k} \operatorname{d}^2(\boldsymbol{x}, \boldsymbol{\mu}_k) + \sum_{\boldsymbol{x} \in P_l} \operatorname{d}^2(\boldsymbol{x}, \boldsymbol{\mu}_l) - \sum_{\boldsymbol{x} \in P_k \cup P_l} \operatorname{d}^2(\boldsymbol{x}, [\boldsymbol{\mu}_k, \boldsymbol{\mu}_l])}{(\#P_k + \#P_l) \lVert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l \rVert^2} \Big]^{1/2}
$$

One can rewrite this : 

$$
a_{kl} =\Big[ \frac{\sum_{\boldsymbol{x} \in P_k} \langle \boldsymbol{x} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k \rangle_+ + \sum_{\boldsymbol{x} \in P_l} \langle \boldsymbol{x} - \boldsymbol{\mu}_l \vert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l\rangle_+}{(\#P_k + \#P_l) \lVert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l \rVert^2} \Big]^{1/2}
$$



Because for each $\boldsymbol{x} \in P_k$, let us denote $\boldsymbol{x}_\bot$ the orthogonal projection on the right line : $(\boldsymbol{\mu}_k, \boldsymbol{\mu}_l)$. 

- If $\boldsymbol{x}_\bot \notin [\boldsymbol{\mu}_k, \boldsymbol{\mu}_l]$, \textit{i.e.} $\langle \boldsymbol{x} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k\rangle < 0$, then the point $\boldsymbol{x}$ is closest to $\boldsymbol{\mu}_k$. In that case, the difference between $\operatorname{d}^2(\boldsymbol{x}, \boldsymbol{\mu}_k) - \operatorname{d}^2(\boldsymbol{x}, [\boldsymbol{\mu}_k, \boldsymbol{\mu}_l]) = 0$.
- If $\boldsymbol{x}_\bot \in [\boldsymbol{\mu}_k, \boldsymbol{\mu}_l]$, \textit{i.e.} $\langle\boldsymbol{x} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k\rangle \geq 0$. And, by the Pythagorean theorem, we have : $\lVert \boldsymbol{x} - \boldsymbol{\mu}_k \rVert^2 = \lVert \boldsymbol{x} - \boldsymbol{x}_\bot \rVert^2 + \lVert \boldsymbol{x}_\bot - \boldsymbol{\mu}_k \rVert^2$, so \\ 
    $\lVert \boldsymbol{x} - \boldsymbol{\mu}_k \rVert^2 - \lVert \boldsymbol{x} - \boldsymbol{x}_\bot \rVert^2 = \lVert \boldsymbol{x}_\bot - \boldsymbol{\mu}_k \rVert^2$ and 
    $\operatorname{d}^2(\boldsymbol{x}, \boldsymbol{\mu}_k) - \operatorname{d}^2(\boldsymbol{x}, [\boldsymbol{\mu}_k, \boldsymbol{\mu}_l]) = \lVert \boldsymbol{x}_\bot - \boldsymbol{\mu}_k \rVert^2 = \langle \boldsymbol{x} - \boldsymbol{\mu}_k, \boldsymbol{\mu}_l - \boldsymbol{\mu}_k \rangle$




Thus, more concisely, $\forall \boldsymbol{x} \in P_k$, one can write :

$$
\operatorname{d}^2(\boldsymbol{x}, \boldsymbol{\mu}_k) - \operatorname{d}^2(\boldsymbol{x}, [\boldsymbol{\mu}_k, \boldsymbol{\mu}_l]) = \langle\boldsymbol{x} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k\rangle_+
$$



## Algorithm



```pseudocode
#| label: alg-spectral-bridges
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Spectral Bridges}
\begin{algorithmic}
\Procedure{SpectralBridges}{$data, k, p$} \Comment{data: input dataset, k: number of clusters, p: number of Voronoi regions}
  \State $centroids \gets$ \Call{KMeans}{$data, p$} \Comment{Initial centroids using k-means}
  \State $voronoiRegions \gets$ \Call{Subdivide}{$data, centroids$} \Comment{Subdivide data into Voronoi regions}
  \State $graph \gets$ \Call{CreateGraph}{$voronoiRegions$} \Comment{Assess connectivity between regions}
  \State $clusters \gets$ \Call{WardLinkage}{$graph, k$} \Comment{Cluster using Ward linkage-inspired approach}
  \State \Return $clusters$
\EndProcedure

\Procedure{KMeans}{$data, p$}
  \State Initialize $p$ centroids randomly
  \Repeat
    \State Assign each point to the nearest centroid
    \State Update centroids based on assignments
  \Until{centroids do not change}
  \State \Return centroids
\EndProcedure

\Procedure{Subdivide}{$data, centroids$}
  \State $voronoiRegions \gets \{\}$
  \For{each point $x$ in $data$}
    \State Find the nearest centroid for $x$
    \State Assign $x$ to the corresponding Voronoi region
  \EndFor
  \State \Return $voronoiRegions$
\EndProcedure

\Procedure{CreateGraph}{$voronoiRegions$}
  \State $graph \gets$ empty graph
  \For{each pair of regions $(R_i, R_j)$ in $voronoiRegions$}
    \State Calculate connectivity measure between $R_i$ and $R_j$
    \State Add edge between $R_i$ and $R_j$ in $graph$ with weight based on connectivity
  \EndFor
  \State \Return $graph$
\EndProcedure

\Procedure{WardLinkage}{$graph, k$}
  \State $clusters \gets$ Initialize each region as a separate cluster
  \Repeat
    \State Find the pair of clusters with the smallest merging cost
    \State Merge the selected pair of clusters
  \Until{number of clusters equals $k$}
  \State \Return $clusters$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```


# Numerical experiments



# References {.unnumbered}

::: {#refs}
:::

# Session information {.appendix .unnumbered}

```{r session-info}
sessionInfo()
```
