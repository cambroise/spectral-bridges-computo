---
title: "Spectral Bridges"
subtitle: "Scalable Spectral Clustering free from hyperparameters"
date: 19/06/2024
date-modified: last-modified
author:
  - name: Félix Laplante
    email: flheight0@gmail.com
    affiliations:
      - Université de Paris Saclay
  - name: Christophe Ambroise
    corresponding: true
    email: christophe.ambroise@univ-evry.fr
    url: https://computo.sfds.asso.fr
    orcid: 0000-0002-8148-0346
    affiliations:
      - name: Université Paris-Saclay, CNRS, Univ Evry, 
        department: Laboratoire de Mathématiques et Modélisation d'Evry
        address: 23 boulevard de France
        city: Evry-Courcouronnes
        country: France
description: |
  This document provides a template based on the quarto system for contributions to Computo. The github repository in itself provides a specific quarto extension useful for authors (and editors!).
keywords: [spectral clustering, vector quantization, scalable, non-parametric]
doi: 10.xxxx/xxx-xxx
citation:
  type: article-journal
  container-title: "Computo"
  doi: "10.xxxx/xxx-xxx"
  url: "https://github.com/computorg/computo-quarto-extension"
  issn: "2824-7795"
bibliography: references.bib
google-scholar: true
github-user: cambroise
repo: "spectral-bridges"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default 
  computo-pdf:  default
---

# Abstract

In this paper, Spectral-Bridges, a novel clustering algorithm, is presented. This algorithm builds upon the traditional k-means and spectral clustering frameworks by subdividing data into small Voronoi regions, which are subsequently assessed for their connectivity. Drawing inspiration from Support vector machine, a non-parametric clustering approach is proposed. This approach is characterized by minimal hyperparameters and intuitive usability, thereby augmenting adaptability and enabling the delineation of intricate, non-convex cluster structures.

Both global and local data arrangements are aimed to be discerned by Spectral-Bridges in a scale-invariant manner. K-means centroids are leveraged for subgroup initialization, and a strategy is proposed to link these regions, with the "affinity" gauged in terms of minimal margin between pairs of classes. 

The empirical results underscore Spectral-Bridges as a fast, robust, and versatile tool for sophisticated clustering tasks spanning diverse domains. Its efficacy is observed to extend seamlessly to large-scale scenarios encompassing both real-world and synthetic datasets.


# Introduction

<!--Interest of clustering-->
Clustering is a fundamental technique for exploratory data analysis. It partition a set of objects into a certain number of homogeneous groups, each referred to as a cluster.  It is extensively utilized across diverse fields such  biology, social sciences, and psychology. Clustering is frequently employed in conjunction with supervised learning as a pre-processing step, where it helps to structure and simplify data, thereby enhancing the performance and interpretability of subsequent predictive models.



<!-- Importance of defining "similar" -->


<!-- Clustering approaches -->

There are numerous approaches to clustering, each defined by how similarity between objects is measured, either through a similarity measure or more strictly through a distance metric.

Density-based methods identify regions within the data that have a high concentration of points, corresponding to the modes of the joint density. A notable non-parametric example of this approach is DBSCAN [@ester1996density]. In contrast, model-based clustering, such as Gaussian mixture models, represents a parametric approach to density-based methods.

Geometric approaches, such as kmeans [@macqueen1967some]  are distance-based and aim to partition the data in a way that optimizes a criterion reflecting group homogeneity.

Graph-based methods treat data as a graph, with vertices representing data points and edges weighted to reflect the affinity between these points.


<!--

Clustering is a fundamental problem in the field of data mining and machine learning [1], whose purpose is to partition a set of objects into a certain number of homogeneous groups, each referred to as a cluster. 
Out of the large number of clustering algorithms that have been developed, spectral clustering in recent years has been gaining increasing attention due to its promising ability in dealing with non- linearly separable datasets [2], [3], [4], [5]. However, a critical limitation to conventional spectral clustering lies in its huge time and space complexity, which significantly restricts its application to large-scale problems.
Conventional spectral clustering typically consists of two time- and memory-consuming phases, namely,
-->



# Background


Spectral clustering is a graph-based approach that computes the eigenvectors of the graph's Laplacian matrix. This technique transforms the data into a lower-dimensional space, making the clusters more discernible. A standard algorithm like k-means is then applied to these transformed features to identify the clusters[@von2007tutorial].

It enables to capture complex data structures and discern clusters based on the connectivity of data points in a transformed space.
Notice that spectral clustering can be seen as a relaxed graph cut problem. 






# Spectral bridges



## Bridge gain version 1

The basic idea involves calculating the reduction in inertia achieved by projecting onto a segment connecting two centroids, rather than using the two centroids separately.


The inertia of two balls $k$ and $l$ is 
$$
I_{kl} = \sum_{i\in k} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2  + \sum_{i\in l} \|\boldsymbol x_i - \boldsymbol \mu_l\|^2.
$$
The inertia of a bridge between $k$ and $l$ is defined as
$$
B_{kl} = \sum_{i\in kl} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2,
$$
where 
$$
\boldsymbol p_{kl}(\boldsymbol x_i) = \boldsymbol \mu_{k} + t_i(\boldsymbol \mu_{l} - \boldsymbol \mu_{k}),
$$
with 
$$
t_i  = \min\left(1, \max\left(0, \frac{\langle \boldsymbol x_i - \boldsymbol \mu_k | \boldsymbol \mu_l - \boldsymbol \mu_k\rangle}{\|  \boldsymbol \mu_l - \boldsymbol \mu_k \|^2}\right)\right). 
$$
Notice that $B_{kl}$ can be expressed as the sum of three terms:
\begin{aligned}
B_{kl} &=& \sum_{i \mid t_i=0} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2  + \sum_{i \mid t_i=1} \|\boldsymbol x_i - \boldsymbol \mu_l\|^2 + \sum_{i \mid t_i\in ]0,1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2.
\end{aligned}
The last term may be decomposed in two parts 
\begin{aligned}
\sum_{i \mid t_i\in ]0,1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 &= &\sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 + \sum_{i \mid t_i\in [\frac{1}{2},1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2\\
\end{aligned}
and each part further decomposed using Pythagore
\begin{aligned}
\sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 &=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol \mu_k - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2\\
&=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|t_i (\boldsymbol \mu_k - \boldsymbol \mu_{l})\|^2,
\end{aligned}

\begin{aligned}
\sum_{i \mid t_i\in ]\frac{1}{2},1[} \|\boldsymbol x_i - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2 &=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol x_i - \boldsymbol \mu_l\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|\boldsymbol \mu_l - \boldsymbol p_{kl}(\boldsymbol x_i)\|^2\\
&=& \sum_{i \mid t_i\in ]\frac{1}{2},1[} \|\boldsymbol x_i - \boldsymbol \mu_k\|^2 - \sum_{i \mid t_i\in ]0,\frac{1}{2}[} \|(1-t_i) (\boldsymbol \mu_k - \boldsymbol \mu_{l})\|^2
\end{aligned}


Thus 
\begin{aligned}
I_{kl}- B_{kl} &=&  \sum_{i \mid t_i\in ]0,\frac{1}{2}[} t_i^2 \|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2 + \sum_{i \mid t_i\in ]\frac{1}{2},1[} (1-t_i)^2 \|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2,\\
\frac{I_{kl}- B_{kl}}{\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2} &=& \sum_{i \mid t_i\in ]0,\frac{1}{2}[} t_i^2  + \sum_{i \mid t_i\in ]\frac{1}{2},1[} (1-t_i)^2 \\
\frac{I_{kl}- B_{kl}}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^2} &=& \frac{\sum_{\boldsymbol{i} \in k} \langle \boldsymbol{x_i} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k \rangle_+^2  \sum_{\boldsymbol{i} \in l} \langle \boldsymbol{x_i} - \boldsymbol{\mu}_l \vert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l\rangle_+^2}{(n_k+n_l)\|\boldsymbol \mu_k - \boldsymbol \mu_l\|^4}
\end{aligned}






## Bridge version 2 

K-means clustering assigns a class to each point based on its proximity to a central point, called a centroid. As a result, the boundary that separates two classes is a hyperplane. This hyperplane is orthogonal to the line segment connecting the centroids of the two classes and intersects this segment at its midpoint.

We introduce the concept of Bridge affinity between two classes, $k$
 and $l$, which is determined by the mean distance of points to the hyperplane that separates these classes. Visualizing the segment as a bridge, we define for each point $\boldsymbol x \in kl$ the distance required to reach the center of the bridge, denoted as  $\boldsymbol \mu_{kl} = \frac{1}{2}(\boldsymbol \mu_k+\boldsymbol \mu_l)$. This formulation draws a parallel to the concept of the support vector margin, emphasizing the geometric characterization of cluster separation. 

Let us denote $\boldsymbol p_{kl}(\boldsymbol x)$ the projection of $\boldsymbol x \in k$ onto the segment $[\boldsymbol \mu_k,\boldsymbol \mu_l]$: 
$$
\boldsymbol p_{kl}(\boldsymbol x) = \boldsymbol \mu_{k} + t(\boldsymbol \mu_{l} - \boldsymbol \mu_{k}),
$$
with 
$$
t  = \min\left(1, \max\left(0, \frac{\langle \boldsymbol x - \boldsymbol \mu_k | \boldsymbol \mu_l - \boldsymbol \mu_k\rangle}{\|  \boldsymbol \mu_l - \boldsymbol \mu_k \|^2}\right)\right). 
$$
The 'bridge distance'  for a point $\boldsymbol x \in k$ is 
$$
\|\boldsymbol p_{kl}(\boldsymbol x) - \boldsymbol \mu_{kl}\| = (1/2-t) \| \boldsymbol \mu_k-\boldsymbol \mu_l \|.
$$
The ratio $\frac{\|\boldsymbol p_{kl}(\boldsymbol x) - \boldsymbol \mu_{kl}\|}{\| \boldsymbol \mu_k-\boldsymbol \mu_k \|}$ acts as a dissmilarity. If the average ratio is large it suggests a low density between the classes. Conversely, if this average  ratio is small, it indicates that the two classes may reside within the same densely populated region.  

We then define the bridge affinity as a fonction of this ratio between the bridge distance and the length of the bridge
$$
 t_{kl}(\boldsymbol x) =  1/2- \frac{\|\boldsymbol p_{kl}(\boldsymbol x) - \boldsymbol \mu_{kl}\|}{\| \boldsymbol \mu_k-\boldsymbol \mu_l \|}.
$$

The bridge affinity between two classes  $k$ and $l$ is defined as the mean bridge affinity of all points of classes  $k$, $l$:

$$
a_{kl} =\sum_{\boldsymbol x \in kl} t_{kl}(\boldsymbol x)= \frac{\sum_{\boldsymbol{x} \in k} \langle \boldsymbol{x} - \boldsymbol{\mu}_k \vert \boldsymbol{\mu}_l - \boldsymbol{\mu}_k \rangle_+ + \sum_{\boldsymbol{x} \in l} \langle \boldsymbol{x} - \boldsymbol{\mu}_l \vert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l\rangle_+}{(\#P_k + \#P_l) \lVert \boldsymbol{\mu}_k - \boldsymbol{\mu}_l \rVert^2}.
$$


## Algorithm



```pseudocode
#| label: alg-spectral-bridges
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Spectral Bridges}
\begin{algorithmic}
\Procedure{SpectralBridges}{$data, k, p$} \Comment{data: input dataset, k: number of clusters, p: number of Voronoi regions}
  \State $centroids \gets$ \Call{KMeans}{$data, p$} \Comment{Initial centroids using k-means}
  \State $voronoiRegions \gets$ \Call{Subdivide}{$data, centroids$} \Comment{Subdivide data into Voronoi regions}
  \State $graph \gets$ \Call{CreateGraph}{$voronoiRegions$} \Comment{Assess connectivity between regions}
  \State $clusters \gets$ \Call{WardLinkage}{$graph, k$} \Comment{Cluster using Ward linkage-inspired approach}
  \State \Return $clusters$
\EndProcedure

\Procedure{KMeans}{$data, p$}
  \State Initialize $p$ centroids randomly
  \Repeat
    \State Assign each point to the nearest centroid
    \State Update centroids based on assignments
  \Until{centroids do not change}
  \State \Return centroids
\EndProcedure

\Procedure{Subdivide}{$data, centroids$}
  \State $voronoiRegions \gets \{\}$
  \For{each point $x$ in $data$}
    \State Find the nearest centroid for $x$
    \State Assign $x$ to the corresponding Voronoi region
  \EndFor
  \State \Return $voronoiRegions$
\EndProcedure

\Procedure{CreateGraph}{$voronoiRegions$}
  \State $graph \gets$ empty graph
  \For{each pair of regions $(R_i, R_j)$ in $voronoiRegions$}
    \State Calculate connectivity measure between $R_i$ and $R_j$
    \State Add edge between $R_i$ and $R_j$ in $graph$ with weight based on connectivity
  \EndFor
  \State \Return $graph$
\EndProcedure

\Procedure{WardLinkage}{$graph, k$}
  \State $clusters \gets$ Initialize each region as a separate cluster
  \Repeat
    \State Find the pair of clusters with the smallest merging cost
    \State Merge the selected pair of clusters
  \Until{number of clusters equals $k$}
  \State \Return $clusters$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```


# Numerical experiments



# References {.unnumbered}

::: {#refs}
:::

# Session information {.appendix .unnumbered}

```{r session-info}
sessionInfo()
```
